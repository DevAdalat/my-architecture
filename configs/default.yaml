model:
  vocab_size: 1000
  hidden_dim: 128
  num_heads: 4
  head_dim: 32
  pool_size: 1024
  pool_dim: 128
  top_k: 128
  knowledge_ratio: 0.5
  reasoning_ratio: 0.3
  grammar_ratio: 0.2

training:
  batch_size: 16
  lr: 0.0001
  epochs: 5
  seq_len: 32
  recurrent_steps: 8
  weight_decay: 0.01

dataset:
  name: "synthetic"
  streaming: false
  column_name: "text"
  prefetch_factor: 2
