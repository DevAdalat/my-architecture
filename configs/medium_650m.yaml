model:
  vocab_size: 32000
  hidden_dim: 1024
  num_layers: 12
  num_heads: 16
  head_dim: 64
  intermediate_dim: 4096
  pool_size: 550000
  pool_dim: 640
  top_k: 512
  retrieval_dim: 128
  max_reasoning_steps: 8
  dropout: 0.1
  knowledge_ratio: 0.7
  reasoning_ratio: 0.2
  grammar_ratio: 0.1
  act_threshold: 0.99
  ponder_lambda: 0.01

training:
  batch_size: 16
  lr: 0.0001
  epochs: 5
  seq_len: 2048
  save_steps: 500
  warmup_steps: 1000

dataset:
  name: "roneneldan/TinyStories"
  tokenizer_name: "EleutherAI/gpt-neo-125m"
  streaming: true
  column_name: "text"
  prefetch_factor: 2
